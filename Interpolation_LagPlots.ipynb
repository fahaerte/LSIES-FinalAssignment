{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from csv import reader\n",
    "from sklearn.metrics import r2_score\n",
    "from datetime import datetime\n",
    "from datetime import timedelta as tdelta\n",
    "from datetime import time as time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from Helper_funcs import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## I. Data import ##"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### I.a) Import positions ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sensor_positions_dir = './data/sensor_positions.csv'\n",
    "stations_df = import_sensor_positions(sensor_positions_dir)\n",
    "IDs = list(stations_df.index)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### I.b) Form groups based on distance between stations ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create groups of sensors based on distance between them\n"
     ]
    }
   ],
   "source": [
    "groups = make_groups(IDs, stations_df)\n",
    "stations_df.insert(loc=1, column='grps', value=groups['grp'])\n",
    "\n",
    "del groups"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### I.c) Import sound pressure values ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dir_path = './data'\n",
    "csv_file_names = []\n",
    "\n",
    "for path in os.listdir(dir_path):\n",
    "    if os.path.isfile(os.path.join(dir_path, path)):\n",
    "        if path.find('data.csv') != -1:\n",
    "            csv_file_names.append(path)\n",
    "            \n",
    "del path, dir_path"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Append all csv files to a list of dataframes and add the id of the sensor to distinguish them later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "list_df = []\n",
    "sensor_names = []\n",
    "\n",
    "\n",
    "for filename in csv_file_names:\n",
    "    df = pd.read_csv('./data/' + filename, index_col=None, header=0)\n",
    "    df['Time'] = df.apply(lambda row: datetime.strptime(row['Time'], '%Y-%m-%d %H:%M:%S'), axis=1)\n",
    "    sensor_names.append(filename[12:16]);\n",
    "    list_df.append(df);\n",
    "\n",
    "list_len = [len(df) for df in list_df];\n",
    "\n",
    "del filename, csv_file_names"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### I.d) Get rid of duplicates and fill in missing timestamps ###\n",
    "-> data is transformed into uniformely sampled data with `np.nan()` in place of missing values\n",
    "-> start date is 2022.02.20 00:00:00, stop date is 2022.03.05 00:00:00\n",
    "-> all data is organised into a DataFrame, columns=Sensor IDs, index=Timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "start_time = datetime.strptime('2022.02.20 00:00:00', '%Y.%m.%d %H:%M:%S')\n",
    "end_time = datetime.strptime('2022.03.05 00:00:00', '%Y.%m.%d %H:%M:%S')\n",
    "tindex = pd.date_range(start_time, end_time, freq='1min')\n",
    "data_label='dt_sound_level_dB'\n",
    "\n",
    "df_data_incomplete = pd.DataFrame(index=tindex, columns=sensor_names)\n",
    "\n",
    "list_df_incomplete = []\n",
    "\n",
    "idx = 0\n",
    "for df in list_df:\n",
    "    # get rid of redundant datapoints\n",
    "    df = df[df.Time >= start_time]\n",
    "    df = df[df.Time <= end_time]\n",
    "    df.drop_duplicates(subset='Time', keep='first', inplace=True)\n",
    "    \n",
    "    # index data by Time\n",
    "    df.index = pd.to_datetime(df['Time'])\n",
    "    df.drop(columns=['Time'], inplace=True)\n",
    "    df = df.reindex(tindex)\n",
    "    \n",
    "    list_df_incomplete.append(df)\n",
    "    df_data_incomplete[sensor_names[idx]] = df[data_label]\n",
    "    idx = idx + 1\n",
    "\n",
    "del df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## II. Interpolation ##"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### II.a) Simple interpolation ###\n",
    "-> 'nearest'\n",
    "-> 'linear'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#interpolate data using 'nearest' method\n",
    "list_df_other = []\n",
    "list_df_linear = []\n",
    "list_df_nearest = []\n",
    "\n",
    "\n",
    "temp_df = df_data_incomplete.interpolate(method='nearest')\n",
    "df_data_nearest = temp_df.interpolate(limit_area=None, method='backfill')\n",
    "  \n",
    "temp_df = df_data_incomplete.interpolate(method='linear')\n",
    "df_data_linear = temp_df.interpolate(limit_area=None, method='backfill') \n",
    "df_data_linear.apply(lambda x: np.round(x, 1), 1);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### II.b) Attempts on advanced interpolation ###"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### II.b.1) Raw data separation and filtering ####\n",
    "-> data is separated into 2 groups: workdays(`df_work_...`), weekenddays(`df_end_...`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "day_str = datetime.strptime('2023.05.01 00:00:00', '%Y.%m.%d %H:%M:%S')\n",
    "day_stp = datetime.strptime('2023.05.01 23:59:59', '%Y.%m.%d %H:%M:%S')\n",
    "daindex = pd.date_range(day_str, day_stp, freq='1min')\n",
    "\n",
    "df_workd_mean = pd.DataFrame(index=daindex, columns=sensor_names)\n",
    "df_endd_mean = pd.DataFrame(index=daindex, columns=sensor_names)\n",
    "\n",
    "df_work_all = df_data_incomplete.loc[df_data_incomplete.index.day_of_week < 5].copy()\n",
    "df_end_all = df_data_incomplete.loc[df_data_incomplete.index.day_of_week > 4].copy()\n",
    "\n",
    "for moment in daindex:\n",
    "    idxs_work = df_work_all.index.indexer_at_time(moment.time())\n",
    "    idxs_end = df_end_all.index.indexer_at_time(moment.time())\n",
    "    \n",
    "    df_workd_mean.loc[moment] = df_work_all.iloc[idxs_work].mean()\n",
    "    df_endd_mean.loc[moment] = df_end_all.iloc[idxs_end].mean()\n",
    "\n",
    "del day_stp, day_str, idxs_work, idxs_end\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### II.b.2) Mean-day model interpolation ####\n",
    "-> data is filtered (meaned) to create average workday and average weekend day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_mean_ww = df_work_all.copy()\n",
    "df_mean_we = df_end_all.copy()\n",
    "\n",
    "for moment in daindex:\n",
    "    idxs_work = df_work_all.index.indexer_at_time(moment.time())\n",
    "    idxs_end = df_end_all.index.indexer_at_time(moment.time())\n",
    "    \n",
    "    df_mean_ww.iloc[idxs_work] = df_workd_mean.loc[moment]\n",
    "    df_mean_we.iloc[idxs_end] = df_endd_mean.loc[moment]\n",
    "    \n",
    "df_mean = pd.concat([df_mean_we, df_mean_ww], axis=0)\n",
    "df_mean.sort_index(inplace=True)\n",
    "\n",
    "del df_mean_ww, df_mean_we, idxs_end, idxs_work"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### II.b.3) Neighbor data interpolation ####\n",
    "- Missing data is filled with data of neighboring sensors\n",
    "-- 1) calculate correlation coefficients\n",
    "--- a) for raw data\n",
    "--- b) for average day\n",
    "-- 2) calculate linear approximation (no offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 1a)\n",
    "sensor_names_set = set(sensor_names);\n",
    "\n",
    "tdata = [[] for _ in range(len(IDs))]\n",
    "stations_df['grp_corrs_i']= tdata\n",
    "icors = []\n",
    "ixx = []\n",
    "iscorr = []\n",
    "\n",
    "for id in sensor_names:\n",
    "    # 1) extract group of sensors\n",
    "    if id in stations_df.index:\n",
    "        group = set(stations_df.loc[id]['grps'])\n",
    "        # 2) filter out non-existing sensors\n",
    "        group = list(sensor_names_set & group)\n",
    "        # 3) extract data\n",
    "        group_df = df_data_incomplete[group]\n",
    "        single_df = df_data_incomplete[id]\n",
    "        stations_df['grps'][id] = group\n",
    "        corrs = []\n",
    "        icors = []\n",
    "        for fid in group:\n",
    "            # 5) determine lag for precision improvement\n",
    "            temp = calculate_correlations(single_df, group_df[fid])\n",
    "            ttemp = list(np.absolute(temp))\n",
    "            itemp = ttemp.index(max(ttemp))\n",
    "            ttuple = (temp[itemp], itemp-10, fid)\n",
    "            corrs.append(ttuple)\n",
    "            icors.append(max(temp))\n",
    "            \n",
    "        stations_df['grp_corrs_i'][id] = corrs\n",
    "        ixx.append(id)\n",
    "        iscorr.append(icors)\n",
    "        #print(corrs)\n",
    "\n",
    "del temp, ttemp, itemp, ttuple, icors, group, corrs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 1b)\n",
    "sensor_names_set = set(sensor_names);\n",
    "\n",
    "tdata = [[] for _ in range(len(IDs))]\n",
    "stations_df['grp_corrs_m']= tdata\n",
    "\n",
    "mcors = []\n",
    "mxx = []\n",
    "\n",
    "mscors = []\n",
    "msxx = []\n",
    "\n",
    "for id in sensor_names:\n",
    "    # 1) extract group of sensors\n",
    "    if id in stations_df.index:\n",
    "        group = set(stations_df.loc[id]['grps'])\n",
    "        # 2) filter out non-existing sensors\n",
    "        group = list(sensor_names_set & group)\n",
    "        # 3) extract data\n",
    "        group_df = df_mean[group]\n",
    "        single_df = df_mean[id]\n",
    "        stations_df['grps'][id] = group\n",
    "        corrs = []\n",
    "        mcors = []\n",
    "        for fid in group:\n",
    "            # 5) determine lag for precision improvement\n",
    "            temp = calculate_correlations(single_df, group_df[fid])\n",
    "            ttemp = list(np.absolute(temp))\n",
    "            itemp = ttemp.index(max(ttemp))\n",
    "            ttuple = (temp(itemp), itemp-10, fid)\n",
    "            mcors.append(max(temp))\n",
    "            corrs.append(ttuple)\n",
    "        stations_df['grp_corrs_m'][id] = corrs\n",
    "        mxx.append(id)\n",
    "        mscors.append(mcors)\n",
    "        #print(corrs)\n",
    "\n",
    "del temp, ttemp, itemp, ttuple, mcors, group, corrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,1)\n",
    "\n",
    "arr0 = []\n",
    "arr1 = []\n",
    "arr2 = []\n",
    "for i in range(0,len(iscorr)):\n",
    "    arr0.append(iscorr[i][0])\n",
    "    if len(iscorr[i]) > 1:\n",
    "        arr1.append(iscorr[i][1])\n",
    "    else:\n",
    "        arr1.append(0)\n",
    "        \n",
    "    if len(iscorr[i]) > 2:\n",
    "        arr2.append(iscorr[i][2])\n",
    "    else:\n",
    "        arr2.append(0)\n",
    "        \n",
    "X = np.arange(len(ixx))\n",
    "\n",
    "ax[0].set_xlabel('station ID')\n",
    "ax[0].set_ylabel('maximum correlation')\n",
    "ax[0].bar(X, arr0, color='r', width = 0.25)\n",
    "ax[0].bar(X-.25, arr1, color='g', width = 0.25)\n",
    "ax[0].bar(X+.25, arr2, color='b', width = 0.25, tick_label=ixx)\n",
    "ax[0].set_title('raw data')\n",
    "\n",
    "\n",
    "\n",
    "arr0 = []\n",
    "arr1 = []\n",
    "arr2 = []\n",
    "for i in range(0,len(mscors)):\n",
    "    arr0.append(mscors[i][0])\n",
    "    if len(mscors[i]) > 1:\n",
    "        arr1.append(mscors[i][1])\n",
    "    else:\n",
    "        arr1.append(0)\n",
    "        \n",
    "    if len(mscors[i]) > 2:\n",
    "        arr2.append(mscors[i][2])\n",
    "    else:\n",
    "        arr2.append(0)\n",
    "        \n",
    "X = np.arange(len(mxx))\n",
    "\n",
    "ax[1].set_xlabel('station ID')\n",
    "ax[1].set_ylabel('maximum correlation')\n",
    "ax[1].bar(X, arr0, color='r', width = 0.25, tick_label=mxx)\n",
    "ax[1].bar(X - .25, arr1, color='g', width = 0.25, tick_label=mxx)\n",
    "ax[1].bar(X + .25, arr2, color='b', width = 0.25, tick_label=mxx)\n",
    "ax[1].set_title('mean day data')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "del X, arr0, arr1, arr2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Raw data clearly shows higher correlation than the averaged one - raw data will be used to fill missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 2)\n",
    "def calculate_linear_regression(df_data1, df_data2, lag):\n",
    "    df_indata = pd.DataFrame(columns=['d1', 'd2'], index=df_data1.index)\n",
    "    df_indata['d1'] = df_data1\n",
    "    df_indata['d2'] = df_data2.shift(periods=lag)\n",
    "    tcov = df_indata.cov(min_periods = 5)\n",
    "    tcov = df_indata.cov(min_periods = 5)\n",
    "    tvar = df_indata['d2'].var()\n",
    "    return tcov/tvar\n",
    "    \n",
    "    \n",
    "    \n",
    "sensor_names_set = set(sensor_names);\n",
    "\n",
    "tdata = [[] for _ in range(len(IDs))]\n",
    "stations_df['grp_linear']= tdata\n",
    "\n",
    "for id in sensor_names:\n",
    "    if id in stations_df.index:\n",
    "        group_ls = stations_df.loc[id]['grp_corrs_i']\n",
    "        single_df = df_data_incomplete[id]\n",
    "        stations_df['grps'][id] = group\n",
    "        beta = []\n",
    "        beta = []\n",
    "        for ftup in group_ls:\n",
    "            # 5) determine lag for precision improvement\n",
    "            fid = ftup[2]\n",
    "            lag = ftup[1]\n",
    "            group_df = df_data_incomplete[fid]\n",
    "            temp = calculate_linear_regression(single_df, group_df, lag)\n",
    "            ttuple = (max(temp), lag, fid)\n",
    "            corrs.append(ttuple)\n",
    "            icors.append(max(temp))\n",
    "            \n",
    "        stations_df['grp_corrs_i'][id] = corrs\n",
    "        ixx.append(id)\n",
    "        iscorr.append(icors)\n",
    "        \n",
    "del "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### II.b.4) Determine resampling period ####\n",
    "\n",
    "Determine resampling period based on rmse=f(Tresample)\n",
    "\n",
    "1) for mean interpolation\n",
    "2) for linear interpolation\n",
    "\n",
    "The data is downsampled for sampling periods of [1, 2, 5, 10, 15, 20, 30, 60] minutes and rmse is calculated to provide an esimate for information loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "1) no interpolation - simple resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "list_rmses = []\n",
    "resample_pers = [1, 2, 5, 10, 15, 20, 30, 60]\n",
    "fig, ax = plt.subplots(2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "no_interpolation_res = evaluate_resample(df_data_incomplete, df_data_incomplete) \n",
    "\n",
    "list_rmses.append([no_interpolation_res[0].mean(),\n",
    "                   no_interpolation_res[1].mean(),\n",
    "                   no_interpolation_res[2].mean()])\n",
    "\n",
    "ax[0, 0] = init_ax_resamp(ax[0, 0], resample_pers, list_rmses)\n",
    "ax[0, 0].set_title('No interpolation,  only downsampling')    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "1) mean interpolation - average days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "mean_interpolation_res = evaluate_resample(df_data_incomplete, df_mean)\n",
    " \n",
    "\n",
    "list_rmses.append([mean_interpolation_res[0].mean(),\n",
    "                   mean_interpolation_res[1].mean(),\n",
    "                   mean_interpolation_res[2].mean()])\n",
    "\n",
    "ax[0, 1] = init_ax_resamp(ax[0, 1], resample_pers, list_rmses)\n",
    "ax[0, 1].set_title('Mean day interpolation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ax[1, 0] = init_ax_resamp(ax[1, 0], resample_pers, list_rmses)\n",
    "ax[1, 1] = init_ax_resamp(ax[1, 1], resample_pers, list_rmses)\n",
    "\n",
    "\n",
    "ax[1, 0].set_title('Region 3')\n",
    "ax[1, 1].set_title('Region 4')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "d = no_interpolation_res[0].copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# TESTING #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "id = sensor_names[0]\n",
    "group = stations_df.loc[id]['grps']\n",
    "colnames = df_data_incomplete.columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_data1 = df_data_incomplete[id]\n",
    "df_data2 = df_data_incomplete[group[0]]\n",
    "df_indata = pd.DataFrame(columns=['d1', 'd2'], index=df_data1.index)\n",
    "df_indata['d1'] = df_data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "len(mscors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ax.bar(X + 0.00, data[0], color = 'b', width = 0.25)\n",
    "ax.bar(X + 0.25, data[1], color = 'g', width = 0.25)\n",
    "ax.bar(X + 0.50, data[2], color = 'r', width = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "len(mxx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "l = np.array()\n",
    "l.append(1)\n",
    "l.append(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for df in [ensemble1, ensemble2]:\n",
    "    mean_list = []\n",
    "    median_list = []\n",
    "    std_list = []\n",
    "\n",
    "    for size in sample_sizes:\n",
    "        sample_size = int(size * len(df))\n",
    "        sample = df.sample(n=sample_size, replace=False)\n",
    "        mean_list.append(np.mean(sample['mean']))\n",
    "        median_list.append(np.median(sample['mean']))\n",
    "        std_list.append(np.std(sample['mean']))\n",
    "\n",
    "    means.append(mean_list)\n",
    "    medians.append(median_list)\n",
    "    stds.append(std_list)\n",
    "\n",
    "fig, ax = plt.subplots(3, figsize=(10, 10))\n",
    "\n",
    "for axis in range(3):\n",
    "    for i, name in enumerate(['ensemble1', 'ensemble2']):\n",
    "        ax[axis].plot(sample_sizes, stat_data[axis][i], label=name)\n",
    "\n",
    "    ax[axis].set_title(stats[axis])\n",
    "    ax[axis].set_xlabel('sample percentage')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}