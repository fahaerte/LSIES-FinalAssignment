{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from geopy import distance\n",
    "from csv import reader\n",
    "from sklearn.metrics import r2_score\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_groups(IDs, stations_df):\n",
    "    # IDs - list of station IDs\n",
    "    # coordinates - array of station's coordinates, \n",
    "    #             - indexable by station ID\n",
    "    # IDs and coordinates are indexed identically\n",
    "    # function returns indexes of grouped stations\n",
    "   \n",
    "    max_distance = 500  # maximum distance for grouped stations in meters\n",
    "    min_distance = 0\n",
    "    print(\"create groups of sensors based on distance between them\")\n",
    "    tdata = np.empty(len(IDs), 'float')\n",
    "    \n",
    "    ds = pd.DataFrame(data={'d': tdata}, index=IDs)\n",
    "    tdata = [[] for _ in range(len(IDs))]\n",
    "    groups_df = pd.DataFrame(data={'grp': tdata}, index=IDs)\n",
    "    \n",
    "    for sid in IDs:\n",
    "        single = stations_df[sid]['coords']\n",
    "        \n",
    "\n",
    "        for sidd in IDs:\n",
    "            tcoord = stations_df.loc[sidd]['coords']\n",
    "            ds.loc[sidd]['d'] = float(distance.distance(single, tcoord).meters)\n",
    "\n",
    "        group = ds.nsmallest(4, 'd')\n",
    "        group = group[group['d'] > min_distance]\n",
    "        group = group[group['d'] < max_distance]\n",
    "        group = list(group.index)\n",
    "        \n",
    "        groups_df.loc[sid]['grp'] = group\n",
    "\n",
    "        \n",
    "    return groups_df.copy()\n",
    "        \n",
    "    \n",
    "def calculate_correlations(group_df_list):\n",
    "    # Function finds intervals of overlapping data, that was actually\n",
    "    # measured by stations in group. The function then calculates \n",
    "    # correlations of measured data for lags from interval [-10; 10].\n",
    "    # These correlations are then averaged over for each lag value \n",
    "    # { ur(lag) = mean(r(lag,i))) } and variance of these values is calculated\n",
    "    # { var(ur) = E[ur(lag) - r(lag,i)] }. Used lag and correlation coeficcient\n",
    "    # is chosen by optimizing = -ru(lag) + var(lag)\n",
    "    \n",
    "    # Function returns chosel lag and correlation coefficient for each partner\n",
    "    # station in the group\n",
    "    \n",
    "    #for df in group_df_list:\n",
    "    pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Task 1) Import all csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['coords'], dtype='object')\n",
      "create groups of sensors based on distance between them\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'2004'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\indexes\\base.py:3652\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3651\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3652\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[0;32m   3653\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\_libs\\index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\_libs\\index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: '2004'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 24\u001b[0m\n\u001b[0;32m     21\u001b[0m IDs \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(stations_df\u001b[39m.\u001b[39mindex)\n\u001b[0;32m     22\u001b[0m \u001b[39mprint\u001b[39m(stations_df\u001b[39m.\u001b[39mcolumns)\n\u001b[1;32m---> 24\u001b[0m groups \u001b[39m=\u001b[39m make_groups(IDs, stations_df)\n\u001b[0;32m     25\u001b[0m stations_df\u001b[39m.\u001b[39minsert(loc\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(stations_df\u001b[39m.\u001b[39mcolumns), column\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mgrps\u001b[39m\u001b[39m'\u001b[39m, value\u001b[39m=\u001b[39mgroups)\n\u001b[0;32m     27\u001b[0m \u001b[39mdel\u001b[39;00m csv_file_name, coords, csv_reader, read_obj, coord, row\n",
      "Cell \u001b[1;32mIn[4], line 18\u001b[0m, in \u001b[0;36mmake_groups\u001b[1;34m(IDs, stations_df)\u001b[0m\n\u001b[0;32m     15\u001b[0m groups_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(data\u001b[39m=\u001b[39m{\u001b[39m'\u001b[39m\u001b[39mgrp\u001b[39m\u001b[39m'\u001b[39m: tdata}, index\u001b[39m=\u001b[39mIDs)\n\u001b[0;32m     17\u001b[0m \u001b[39mfor\u001b[39;00m sid \u001b[39min\u001b[39;00m IDs:\n\u001b[1;32m---> 18\u001b[0m     single \u001b[39m=\u001b[39m stations_df[sid][\u001b[39m'\u001b[39m\u001b[39mcoords\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m     21\u001b[0m     \u001b[39mfor\u001b[39;00m sidd \u001b[39min\u001b[39;00m IDs:\n\u001b[0;32m     22\u001b[0m         tcoord \u001b[39m=\u001b[39m stations_df\u001b[39m.\u001b[39mloc[sidd][\u001b[39m'\u001b[39m\u001b[39mcoords\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\frame.py:3760\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3758\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   3759\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3760\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[0;32m   3761\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3762\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\indexes\\base.py:3654\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3652\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3653\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m-> 3654\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m   3655\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m   3656\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3657\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3658\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3659\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: '2004'"
     ]
    }
   ],
   "source": [
    "csv_file_name = 'sensor_positions.csv'\n",
    "coords = []\n",
    "IDs = []\n",
    "\n",
    "with open('./data/' + csv_file_name, 'r') as read_obj:\n",
    "    csv_reader = reader(read_obj)\n",
    "    for row in csv_reader:\n",
    "        coord = (float(row[0].split(' ')[0].replace('(', '')), \n",
    "                 float(row[0].split(' ')[1].replace(')', '')) )\n",
    "        coords.append(coord)\n",
    "        \n",
    "        IDs.append(row[1])\n",
    "\n",
    "\n",
    "stations_df = pd.DataFrame(data={'coords': coords, 'IDs': IDs})\n",
    "stations_df.drop_duplicates(subset='IDs', keep='first', inplace=True)\n",
    "stations_df.set_index('IDs', inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "IDs = list(stations_df.index)\n",
    "\n",
    "groups = make_groups(IDs, stations_df)\n",
    "stations_df.insert(loc=1, column='grps', value=groups)\n",
    "\n",
    "del csv_file_name, coords, csv_reader, read_obj, coord, row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dir_path = './data'\n",
    "csv_file_names = []\n",
    "\n",
    "for path in os.listdir(dir_path):\n",
    "    if os.path.isfile(os.path.join(dir_path, path)):\n",
    "        if path.find('.csv') != -1:\n",
    "            csv_file_names.append(path)\n",
    "            \n",
    "del path, dir_path, csv_file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Append all csv files to a list of dataframes and add the id of the sensor to distinguish them later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "list_df = []\n",
    "sensor_names = []\n",
    "\n",
    "for filename in csv_file_names:\n",
    "    df = pd.read_csv('./data/' + filename, index_col=None, header=0)\n",
    "    df['Time'] = df.apply(lambda row: datetime.strptime(row['Time'], '%Y-%m-%d %H:%M:%S'), axis=1)\n",
    "    sensor_names.append(filename[12:16]);\n",
    "    list_df.append(df);\n",
    "\n",
    "list_len = [len(df) for df in list_df];"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ls = distance.distance(sensor_df['coords'][0], sensor_df['coords']).meters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.strptime('2022.02.20 00:00:00', '%Y.%m.%d %H:%M:%S')\n",
    "end_time = datetime.strptime('2022.03.05 00:00:00', '%Y.%m.%d %H:%M:%S')\n",
    "index = pd.date_range(start_time, end_time, freq='1min')\n",
    "\n",
    "list_df_incomplete = []\n",
    "\n",
    "\n",
    "for df in list_df:\n",
    "    # get rid of redundant datapoints\n",
    "    df = df[df.Time >= start_time]\n",
    "    df = df[df.Time <= end_time]\n",
    "    df.drop_duplicates(subset='Time', keep='first', inplace=True)\n",
    "    \n",
    "    # index data by Time\n",
    "    df.index = pd.to_datetime(df['Time'])\n",
    "    df.drop(columns=['Time'], inplace=True)\n",
    "    df = df.reindex(index)\n",
    "    \n",
    "    list_df_incomplete.append(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Remove all measurements with a timestamp not inbetween 2022.02.20 00:00:00 to 2022.03.04 24:00:00, set the timestamp as index of the dataframe, and add the start and end timestamp of the measurement period to the dataframe it doesn't exist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Convert the time series to equally-spaced data with one minute time intervals and interpolate missing data with the \"nearest\" interpolant. Therefore, let's insert all missing timestamps first, join existing measurements and finally interpolate missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#interpolate data using 'nearest' method\n",
    "list_df_other = []\n",
    "list_df_nearest = []\n",
    "\n",
    "for df in list_df:\n",
    "    df = df.interpolate(method='nearest')\n",
    "    list_df_nearest.append(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Task 2) Interpolate missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Resample to eliminate rows not corresponding to our sampling frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Task 3) Construct Lag plots for lags 1, 10, 60 and 720 for one sensor (for all sensors it would be 48 plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "lags = [1, 10, 60, 720]\n",
    "figure, axis = plt.subplots(4, 1, figsize=(10, 10))\n",
    "\n",
    "for i, lag in enumerate(lags):\n",
    "    pd.plotting.lag_plot(list_df_inter[0]['dt_sound_level_dB_y'], lag=lag, ax=axis[i])\n",
    "    axis[i].set_xlabel('dB(t)')\n",
    "    axis[i].set_ylabel(f'dB(t + {lag} minutes)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Compute R2 values for each lag plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for lag in lags:\n",
    "    for i, df in enumerate(list_df_inter):\n",
    "        y_pred = df['dt_sound_level_dB_y'].iloc[lag:]\n",
    "        y_true = df['dt_sound_level_dB_y'].iloc[:-lag]\n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "        print(f'R2 coefficient for sensor {sensor_names[i]} and lag {lag}: {r2}')\n",
    "\n",
    "    print('-----------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Task 4) Interpretation of R2 coefficients:\n",
    "It appears that there is a strong positive correlation between the sound level measured by each sensor and the lagged sound level at a lag of 1 minute, with R2 values ranging sligthly below 1. As the lag increases, the correlation becomes weaker, with R2 values ranging around 0.5 for a lag of 60 minutes. For a lag of 720 minutes, there is a negative correlation, which indicate that the model doesn't follow the trend of data at all (switched to non-deterministic)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Task 5) Randomly resample the data from three stations using a uniform distribution with sample sizes of 1%, 5%, 10%, 50% and 90%. Plot the three statistical parameters mean, median and standard deviation as a function of the sample percentage for each site.\n",
    "Stations: 206D, 201D (next to each other), 2004 (further away)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sample_sizes = [0.01, 0.05, 0.1, 0.5, 0.9]\n",
    "stats = ['mean', 'median', 'std']\n",
    "\n",
    "data = [list_df_inter[4], list_df_inter[3], list_df_inter[0]]\n",
    "sensor_names = ['206D', '201D', '2004']\n",
    "means = []\n",
    "medians = []\n",
    "stds = []\n",
    "\n",
    "for df in data:\n",
    "    mean_list = []\n",
    "    median_list = []\n",
    "    std_list = []\n",
    "\n",
    "    for size in sample_sizes:\n",
    "        sample_size = int(size * len(df))\n",
    "        sample = df.sample(n=sample_size, replace=False)\n",
    "        mean_list.append(np.mean(sample['dt_sound_level_dB_y']))\n",
    "        median_list.append(np.median(sample['dt_sound_level_dB_y']))\n",
    "        std_list.append(np.std(sample['dt_sound_level_dB_y']))\n",
    "\n",
    "    means.append(mean_list)\n",
    "    medians.append(median_list)\n",
    "    stds.append(std_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, figsize=(10, 10))\n",
    "stat_data = [means, medians, stds]\n",
    "\n",
    "for axis in range(3):\n",
    "    for i, name in enumerate(sensor_names):\n",
    "        ax[axis].plot(sample_sizes, stat_data[axis][i], label=name)\n",
    "\n",
    "    ax[axis].set_title(stats[axis])\n",
    "    ax[axis].set_xlabel('sample percentage')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Plot the same three statistical parameters, but as ensembles\n",
    "1. Ensemble: 206D, 2004 (far from each other)\n",
    "2. All three\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ensemble1 = data[0].merge(data[1], how='left', on='Time')\n",
    "ensemble2 = ensemble1.merge(data[2], how='left', on='Time')\n",
    "ensemble1['mean'] = ensemble1[['dt_sound_level_dB_y_x', 'dt_sound_level_dB_y_y']].mean(axis=1)\n",
    "ensemble2['mean'] = ensemble2[['dt_sound_level_dB_y_x', 'dt_sound_level_dB_y_y', 'dt_sound_level_dB_y']].mean(axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for df in [ensemble1, ensemble2]:\n",
    "    mean_list = []\n",
    "    median_list = []\n",
    "    std_list = []\n",
    "\n",
    "    for size in sample_sizes:\n",
    "        sample_size = int(size * len(df))\n",
    "        sample = df.sample(n=sample_size, replace=False)\n",
    "        mean_list.append(np.mean(sample['mean']))\n",
    "        median_list.append(np.median(sample['mean']))\n",
    "        std_list.append(np.std(sample['mean']))\n",
    "\n",
    "    means.append(mean_list)\n",
    "    medians.append(median_list)\n",
    "    stds.append(std_list)\n",
    "\n",
    "fig, ax = plt.subplots(3, figsize=(10, 10))\n",
    "\n",
    "for axis in range(3):\n",
    "    for i, name in enumerate(['ensemble1', 'ensemble2']):\n",
    "        ax[axis].plot(sample_sizes, stat_data[axis][i], label=name)\n",
    "\n",
    "    ax[axis].set_title(stats[axis])\n",
    "    ax[axis].set_xlabel('sample percentage')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
